{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/aperto/anaconda/envs/feral/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from model import HFModel\n",
    "\n",
    "\n",
    "with open('configs/ants.yaml', 'r') as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "model = HFModel(model_name=cfg['model_name'], num_classes=cfg['num_classes'], predict_per_item=cfg['predict_per_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4409,  0.7933,  0.8316],\n",
       "        [-0.5529,  0.7082,  0.8037],\n",
       "        [-0.4003,  0.6543,  0.7291],\n",
       "        [-0.4463,  0.6978,  0.7601],\n",
       "        [-0.6452,  0.5759,  0.5980],\n",
       "        [-0.4590,  0.6769,  0.6738],\n",
       "        [-0.5591,  0.4920,  0.7761],\n",
       "        [-0.4273,  0.4845,  0.6166],\n",
       "        [-0.3632,  0.6281,  0.7941],\n",
       "        [-0.4657,  0.5509,  0.7245],\n",
       "        [-0.4006,  0.6374,  0.7626],\n",
       "        [-0.4391,  0.5453,  0.8259],\n",
       "        [-0.4009,  0.6585,  0.7256],\n",
       "        [-0.4320,  0.7479,  1.0331],\n",
       "        [-0.3193,  0.8255,  0.9509],\n",
       "        [-0.3624,  0.5236,  0.7024],\n",
       "        [-0.1114, -0.0171, -0.0689],\n",
       "        [-0.2205, -0.0874, -0.1469],\n",
       "        [-0.1114, -0.1832, -0.2046],\n",
       "        [-0.1241, -0.1262, -0.1425],\n",
       "        [-0.3375, -0.2134, -0.2999],\n",
       "        [-0.1197, -0.1408, -0.2865],\n",
       "        [-0.2664, -0.3375, -0.1388],\n",
       "        [-0.1754, -0.2948, -0.3379],\n",
       "        [-0.0944, -0.2247, -0.1270],\n",
       "        [-0.1765, -0.2509, -0.2232],\n",
       "        [-0.0756, -0.1913, -0.1406],\n",
       "        [-0.1658, -0.2632, -0.1722],\n",
       "        [-0.1356, -0.1499, -0.2223],\n",
       "        [-0.1152, -0.0133,  0.0795],\n",
       "        [-0.0259, -0.0265,  0.0266],\n",
       "        [-0.0916, -0.2776, -0.2065],\n",
       "        [ 0.4270,  0.3004, -0.1438],\n",
       "        [ 0.3065,  0.2352, -0.2328],\n",
       "        [ 0.3841,  0.1731, -0.3105],\n",
       "        [ 0.3941,  0.2176, -0.2706],\n",
       "        [ 0.1909,  0.1217, -0.3815],\n",
       "        [ 0.4028,  0.2071, -0.3486],\n",
       "        [ 0.2846, -0.0076, -0.2379],\n",
       "        [ 0.3843,  0.0073, -0.4340],\n",
       "        [ 0.4750,  0.1064, -0.2415],\n",
       "        [ 0.3407,  0.1001, -0.2453],\n",
       "        [ 0.4504,  0.1221, -0.2408],\n",
       "        [ 0.3237,  0.0997, -0.1971],\n",
       "        [ 0.4120,  0.1856, -0.3508],\n",
       "        [ 0.3804,  0.2974, -0.0346],\n",
       "        [ 0.5064,  0.3148, -0.1148],\n",
       "        [ 0.4335,  0.0428, -0.3236],\n",
       "        [ 0.1169, -0.3928, -0.3466],\n",
       "        [-0.0164, -0.4541, -0.4038],\n",
       "        [ 0.1658, -0.5735, -0.4710],\n",
       "        [ 0.0707, -0.5162, -0.4110],\n",
       "        [-0.0931, -0.6166, -0.5613],\n",
       "        [ 0.0871, -0.5316, -0.5144],\n",
       "        [-0.0357, -0.7096, -0.4235],\n",
       "        [ 0.1371, -0.7030, -0.5869],\n",
       "        [ 0.1803, -0.5902, -0.4106],\n",
       "        [ 0.0758, -0.6685, -0.4783],\n",
       "        [ 0.1349, -0.5306, -0.4826],\n",
       "        [ 0.0438, -0.6786, -0.3769],\n",
       "        [ 0.1163, -0.5222, -0.5080],\n",
       "        [ 0.0615, -0.3950, -0.1641],\n",
       "        [ 0.1959, -0.3897, -0.2646],\n",
       "        [ 0.1312, -0.7111, -0.5206]], device='cuda:2',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:2')\n",
    "model = model.to(device)\n",
    "\n",
    "x = torch.rand(4, 8, 3, 128, 128, device=device)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Save the original function\n",
    "original_sdp_attention = F.scaled_dot_product_attention\n",
    "original_forward = nn.MultiheadAttention.forward\n",
    "original_mha_fwd = F.multi_head_attention_forward\n",
    "\n",
    "# Define the wrapper\n",
    "def patched_sdp_attention(query, key, value, *args, **kwargs):\n",
    "    print(\"scaled_dot_product_attention called! Query shape:\", query.shape)\n",
    "    return original_sdp_attention(query, key, value, *args, **kwargs)\n",
    "\n",
    "F.scaled_dot_product_attention = patched_sdp_attention\n",
    "\n",
    "def debug_forward(self, *args, **kwargs):\n",
    "    print(\"MultiheadAttention forward called\")\n",
    "    return original_forward(self, *args, **kwargs)\n",
    "\n",
    "nn.MultiheadAttention.forward = debug_forward\n",
    "\n",
    "def debug_mha_fwd(*args, **kwargs):\n",
    "    print(\"multi_head_attention_forward\")\n",
    "    return original_mha_fwd(*args, **kwargs)\n",
    "\n",
    "F.multi_head_attention_forward = debug_mha_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 8, 3, 128, 128, device=device)\n",
    "with torch.backends.cuda(enable_math=False, enable_mem_efficient=False), torch.amp.autocast(dtype=torch.bfloat16, device_type=\"cuda\"):\n",
    "    model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HFModel(\n",
       "  (model): SmolVLMVisionTransformer(\n",
       "    (embeddings): SmolVLMVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "      (position_embedding): Embedding(1024, 768)\n",
       "    )\n",
       "    (encoder): SmolVLMEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SmolVLMEncoderLayer(\n",
       "          (self_attn): SmolVLMVisionAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SmolVLMVisionMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (clip_projector): AttentionPoolingBlockCustom(\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_norm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc_dropout): Identity()\n",
       "  (head): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.backends.cuda.math_sdp_enabled()\n",
    "torch.backends.cuda.flash_sdp_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mha.get_fastpath_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cuda.mem_efficient_sdp_enabled(), torch.backends.cuda.math_sdp_enabled(), torch.backends.cuda.flash_sdp_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can_use_flash_attention() missing 1 required positional argument: 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcan_use_flash_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can_use_flash_attention() missing 1 required positional argument: 'params'"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.can_use_flash_attention()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
